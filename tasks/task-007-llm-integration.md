# Task 007: LLM Integration

**Phase**: 3 - Analysis Engine  
**Estimated Hours**: 5  
**Priority**: P2  
**Status**: Not Started

---

## Description

Integrate Large Language Model (LLM) functionality into the idea-planner-agent for advanced analysis capabilities. This task implements the abstract LLM provider interface and integrates it with the Telegram bot.

---

## Acceptance Criteria

- [ ] Abstract `LLMProvider` interface implemented (FR-008)
- [ ] `GroqProvider` implementation working with Llama-3.3-70b (Architecture Decision)
- [ ] Provider configuration system functional (FR-008)
- [ ] Fallback mechanism for LLM failures implemented (Resilience VII)
- [ ] LLM response validation layer working (Engineering Quality VI)
- [ ] Integration with Telegram bot completed
- [ ] All LLM operations respect ethical guidelines (Ethics VIII)

---

## Subtasks with Hour Estimates

| Subtask | Hours | Description |
|---------|-------|-------------|
| 7.1 Design LLM interface | 1.0 | Create abstract LLMProvider interface |
| 7.2 Implement Groq provider | 2.0 | Groq API integration with Llama-3.3-70b |
| 7.3 Add configuration system | 0.5 | Provider selection and configuration |
| 7.4 Implement fallback mechanism | 1.0 | Graceful degradation on failures |
| 7.5 Add validation layer | 0.5 | Response validation and error handling |

---

## Dependencies

**Depends on**: 
- Task 003 (Configuration System) - for API keys and settings
- Task 006 (Telegram Bot Logic) - for integration points

**Required for**: 
- Task 008 (Mode Analysis) - uses LLM for advanced analysis

---

## Testing Requirements

- [ ] Verify LLM interface works with multiple providers
- [ ] Test Groq provider handles API errors gracefully
- [ ] Confirm configuration system switches providers correctly
- [ ] Validate fallback mechanism works on LLM failures
- [ ] Test response validation catches invalid outputs
- [ ] Verify integration with Telegram bot works smoothly

---

## Traceability to Constitution Principles

| Subtask | Constitution Principle | Spec Reference |
|---------|-----------------------|----------------|
| LLM interface | Engineering Quality (VI) | FR-008 |
| Groq provider | Reality-First (III) | Architecture Decision |
| Configuration | Traceability (II) | FR-008 |
| Fallback mechanism | Resilience (VII) | US-4 |
| Validation layer | Ethics (VIII) | AC-4 |

---

## Implementation Notes

### Abstract LLM Provider Interface

```python
# llm/providers/base.py
from abc import ABC, abstractmethod
from typing import Dict, List, Optional
from pydantic import BaseModel

class LLMRequest(BaseModel):
    """Standardized LLM request format"""
    prompt: str
    temperature: float = 0.7
    max_tokens: int = 1024
    system_message: Optional[str] = None
    context: Optional[Dict] = None

class LLMResponse(BaseModel):
    """Standardized LLM response format"""
    content: str
    usage: Dict[str, int]
    model: str
    provider: str
    raw_response: Optional[Dict] = None

class LLMProvider(ABC):
    """Abstract base class for LLM providers"""
    
    @abstractmethod
    def __init__(self, api_key: str, model: str = "default"):
        """Initialize provider with API key"""
        pass
    
    @abstractmethod
    async def generate(
        self,
        request: LLMRequest,
        retry: int = 3
    ) -> LLMResponse:
        """
        Generate response from LLM
        
        Args:
            request: LLMRequest with prompt and parameters
            retry: Number of retry attempts
            
        Returns:
            LLMResponse with generated content
            
        Raises:
            LLMError: If generation fails after retries
        """
        pass
    
    @abstractmethod
    def validate_response(self, response: LLMResponse) -> bool:
        """
        Validate LLM response for quality and safety
        
        Args:
            response: LLMResponse to validate
            
        Returns:
            bool: True if response is valid
        """
        pass
    
    @abstractmethod
    def get_capabilities(self) -> Dict:
        """
        Return provider capabilities
        
        Returns:
            Dict with max_tokens, models, etc.
        """
        pass

class LLMError(Exception):
    """Custom exception for LLM errors"""
    pass
```

### Groq Provider Implementation

```python
# llm/providers/groq.py
import os
import asyncio
from typing import Dict, Optional
from groq import AsyncGroq
from .base import LLMProvider, LLMRequest, LLMResponse, LLMError

class GroqProvider(LLMProvider):
    """Groq API provider implementation"""
    
    def __init__(self, api_key: str = None, model: str = "llama-3.3-70b"):
        """
        Initialize Groq provider
        
        Args:
            api_key: Groq API key
            model: Model name (default: llama-3.3-70b)
        """
        self.api_key = api_key or os.getenv("GROQ_API_KEY") or os.getenv("LLM_API_KEY")
        self.model = model
        self.client = AsyncGroq(api_key=self.api_key)
        
        if not self.api_key:
            raise ValueError("Groq API key is required")
    
    async def generate(
        self,
        request: LLMRequest,
        retry: int = 3
    ) -> LLMResponse:
        """
        Generate response using Groq API
        
        Acceptance Criteria:
            - FR-008: Configurable LLM provider
            - Resilience VII: Retry mechanism
        """
        last_error = None
        
        for attempt in range(retry):
            try:
                # Prepare messages
                messages = []
                if request.system_message:
                    messages.append({"role": "system", "content": request.system_message})
                messages.append({"role": "user", "content": request.prompt})
                
                # Make API call
                response = await self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=request.temperature,
                    max_tokens=request.max_tokens,
                )
                
                # Create standardized response
                llm_response = LLMResponse(
                    content=response.choices[0].message.content,
                    usage={
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    model=self.model,
                    provider="groq",
                    raw_response=response.model_dump()
                )
                
                # Validate response
                if not self.validate_response(llm_response):
                    raise LLMError("Invalid LLM response")
                
                return llm_response
                
            except Exception as e:
                last_error = e
                if attempt < retry - 1:
                    await asyncio.sleep(2 ** attempt)  # Exponential backoff
                continue
        
        raise LLMError(f"Failed after {retry} attempts: {str(last_error)}")
    
    def validate_response(self, response: LLMResponse) -> bool:
        """
        Validate Groq response
        
        Acceptance Criteria:
            - AC-4: Data accuracy validation
            - Ethics VIII: Content safety
        """
        # Check for empty response
        if not response.content or len(response.content.strip()) < 10:
            return False
        
        # Check for inappropriate content (basic check)
        inappropriate_terms = ["error", "cannot", "unable", "sorry"]
        if any(term in response.content.lower() for term in inappropriate_terms):
            return False
        
        return True
    
    def get_capabilities(self) -> Dict:
        """Return Groq provider capabilities"""
        return {
            "max_tokens": 8192,
            "models": ["llama-3.3-70b", "llama-3.1-70b", "llama-3.1-8b"],
            "concurrent_requests": 10,
            "supports_streaming": True
        }
```

### LLM Provider Factory

```python
# llm/factory.py
from typing import Dict, Type
from .providers.base import LLMProvider
from .providers.groq import GroqProvider

class LLMProviderFactory:
    """Factory for creating LLM providers"""
    
    PROVIDERS: Dict[str, Type[LLMProvider]] = {
        "groq": GroqProvider,
        # Add other providers here
    }
    
    @staticmethod
    def create_provider(
        provider_name: str,
        api_key: str = None,
        model: str = None
    ) -> LLMProvider:
        """
        Create LLM provider instance
        
        Args:
            provider_name: Name of provider (groq, etc.)
            api_key: API key for provider
            model: Specific model to use
            
        Returns:
            LLMProvider instance
            
        Raises:
            ValueError: If provider not supported
        """
        provider_class = LLMProviderFactory.PROVIDERS.get(provider_name.lower())
        
        if not provider_class:
            raise ValueError(f"Unsupported provider: {provider_name}")
        
        return provider_class(api_key=api_key, model=model)
```

### LLM Integration with Telegram Bot

```python
# bot/llm_integration.py
from llm.factory import LLMProviderFactory
from llm.providers.base import LLMRequest, LLMResponse, LLMError
from config import config
import logging

logger = logging.getLogger(__name__)

class LLMIntegration:
    """LLM integration for Telegram bot"""
    
    def __init__(self):
        self.provider = None
        self._initialize_provider()
    
    def _initialize_provider(self):
        """Initialize LLM provider from config"""
        try:
            self.provider = LLMProviderFactory.create_provider(
                provider_name=config.LLM_PROVIDER,
                api_key=config.LLM_API_KEY,
                model=config.LLM_MODEL
            )
            logger.info(f"Initialized LLM provider: {config.LLM_PROVIDER}")
        except Exception as e:
            logger.error(f"Failed to initialize LLM provider: {e}")
            self.provider = None
    
    async def generate_analysis(
        self,
        idea: str,
        mode: str,
        context: dict = None
    ) -> str:
        """
        Generate analysis using LLM
        
        Args:
            idea: Business idea text
            mode: Analysis mode
            context: Additional context data
            
        Returns:
            Generated analysis text
            
        Acceptance Criteria:
            - FR-008: LLM integration working
            - Resilience VII: Fallback on failure
        """
        if not self.provider:
            logger.warning("No LLM provider available, using fallback")
            return self._fallback_analysis(idea, mode)
        
        try:
            # Create mode-specific prompt
            prompt = self._create_prompt(idea, mode, context)
            
            # Create LLM request
            request = LLMRequest(
                prompt=prompt,
                temperature=0.7,
                max_tokens=2048,
                system_message=self._get_system_message(mode)
            )
            
            # Generate response
            response = await self.provider.generate(request)
            
            # Validate response
            if not self._validate_llm_response(response):
                raise LLMError("Invalid LLM response")
            
            return response.content
            
        except LLMError as e:
            logger.error(f"LLM generation failed: {e}")
            return self._fallback_analysis(idea, mode)
        except Exception as e:
            logger.error(f"Unexpected LLM error: {e}")
            return self._fallback_analysis(idea, mode)
    
    def _create_prompt(self, idea: str, mode: str, context: dict) -> str:
        """Create mode-specific prompt"""
        base_prompt = f"""
        –í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑—É –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ —Ä—ã–Ω–∫–∞.
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â—É—é –±–∏–∑–Ω–µ—Å-–∏–¥–µ—é –≤ —Ä–µ–∂–∏–º–µ {mode}:
        
        –ò–¥–µ—è: {idea}
        
        –ö–æ–Ω—Ç–µ–∫—Å—Ç:
        {context.get('market_data', '–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö')}
        
        –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
        1. –û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–æ—Ä–º–∞—Ç Markdown
        3. –í–∫–ª—é—á–∏—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ü–∏—Ñ—Ä—ã –∏ —Ñ–∞–∫—Ç—ã
        4. –£–∫–∞–∂–∏—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [URL, DD.MM.YYYY HH:MM, "–æ–ø–∏—Å–∞–Ω–∏–µ"]
        5. –†–∞–∑–¥–µ–ª–∏—Ç–µ –æ—Ç–≤–µ—Ç –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∞–±–∑–∞—Ü—ã (–º–∞–∫—Å–∏–º—É–º 3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∫–∞–∂–¥—ã–π)
        """
        
        # Add mode-specific instructions
        if mode == "–ë–ò–ó–ù–ï–°-–ü–õ–ê–ù":
            base_prompt += "\n6. –°—Ñ–æ–∫—É—Å–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è—Ö –∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω–æ–π –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
        elif mode == "–ú–ê–†–ö–ï–¢–ò–ù–ì":
            base_prompt += "\n6. –£–¥–µ–ª–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –∏ –∫–∞–Ω–∞–ª–∞–º –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è"
        # ... other modes
        
        return base_prompt
    
    def _get_system_message(self, mode: str) -> str:
        """Get system message for mode"""
        system_messages = {
            "–û–¶–ï–ù–ö–ê": "–í—ã - —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫",
            "–ë–ò–ó–ù–ï–°-–ü–õ–ê–ù": "–í—ã - —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —ç–∫—Å–ø–µ—Ä—Ç, –≥–æ—Ç–æ–≤–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∏–Ω–≤–µ—Å—Ç–æ—Ä–æ–≤",
            "–ú–ê–†–ö–ï–¢–ò–ù–ì": "–í—ã - –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–æ–≤—ã–π —Å—Ç—Ä–∞—Ç–µ–≥",
            "–ò–°–ü–û–õ–ù–ï–ù–ò–ï": "–í—ã - –æ–ø—ã—Ç–Ω—ã–π –ø—Ä–æ–µ–∫—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä",
            "–°–ê–ô–¢": "–í—ã - –≤–µ–±-—Å—Ç—Ä–∞—Ç–µ–≥ –∏ UX-—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç",
            "–û–¢–ß–Å–¢ 1": "–í—ã - –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö, —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞–º",
            "–û–¢–ß–Å–¢ 2": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é",
            "–û–¢–ß–Å–¢ 3": "–í—ã - —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫",
            "–ü–û–ß–ï–ú–£_–°–ï–ô–ß–ê–°": "–í—ã - –∞–Ω–∞–ª–∏—Ç–∏–∫ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤",
            "–†–´–ù–û–ß–ù–´–ô_–†–ê–ó–†–´–í": "–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É",
            "–î–û–ö–ê–ó–ê–¢–ï–õ–¨–°–¢–í–ê": "–í—ã - –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å –¥–∞–Ω–Ω—ã—Ö"
        }
        
        return system_messages.get(mode, "–í—ã - –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫")
    
    def _validate_llm_response(self, response: LLMResponse) -> bool:
        """Validate LLM response quality"""
        # Check minimum length
        if len(response.content.strip()) < 100:
            return False
        
        # Check for hallucination indicators
        hallucination_terms = ["–Ω–µ –º–æ–≥—É", "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏", "–Ω–µ –∑–Ω–∞—é"]
        if any(term in response.content.lower() for term in hallucination_terms):
            return False
        
        return True
    
    def _fallback_analysis(self, idea: str, mode: str) -> str:
        """
        Fallback analysis when LLM unavailable
        
        Acceptance Criteria:
            - Resilience VII: Graceful degradation
        """
        logger.warning("Using fallback analysis for LLM")
        
        # Basic fallback response
        fallback = f"""
        üìä –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∏–¥–µ–∏: {idea}
        
        **–†–µ–∂–∏–º:** {mode}
        
        ‚ö†Ô∏è –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –í–æ—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã:
        
        1. **–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –∏–¥–µ–∏:** –°—Ä–µ–¥–Ω–∏–π
        2. **–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è:** –ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–∏ 25-45 –ª–µ—Ç
        3. **–ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è:** –í—ã—Å–æ–∫–∞—è
        4. **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:** –ü—Ä–æ–≤–µ–¥–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä—ã–Ω–∫–∞
        
        üîÑ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
        """
        
        return fallback
```

### Integration with Report Generator

```python
# Update to bot/report_generator.py
from bot.llm_integration import LLMIntegration

class ReportGenerator:
    def __init__(self):
        self.data_cache = {}
        self.llm_integration = LLMIntegration()  # Add LLM integration
    
    async def generate_report(
        self,
        idea_text: str,
        mode: str = "–û–¶–ï–ù–ö–ê",
        progress_callback=None
    ) -> List[str]:
        """Updated to use LLM integration"""
        # ... existing code ...
        
        # Get market data
        search_results = search(idea_text, sources=["wb", "ozon", "yandex"])
        
        # Get LLM analysis for mode-specific sections
        llm_context = {
            'market_data': self._format_market_data(search_results),
            'mode': mode,
            'idea': idea_text
        }
        
        llm_analysis = await self.llm_integration.generate_analysis(
            idea_text, 
            mode, 
            llm_context
        )
        
        # Use LLM analysis for appropriate sections
        sections = []
        
        # Section 1: Use LLM for comprehensive analysis
        sections.append(self._generate_section_1_with_llm(idea_text, llm_analysis, mode))
        
        # ... other sections using LLM analysis where appropriate
        
        return self._split_for_telegram("\n\n".join(sections))
    
    def _generate_section_1_with_llm(self, idea: str, llm_analysis: str, mode: str) -> str:
        """Generate Section 1 using LLM analysis"""
        # Extract relevant parts from LLM analysis
        section = f"üìã –ö–ê–†–¢–û–ß–ö–ê –ò–î–ï–ò (–ê–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ò–ò)\n\n"
        section += llm_analysis  # Use full LLM analysis for this section
        
        return section
```

---

## Success Criteria

- [ ] LLM provider interface implemented and working
- [ ] Groq provider successfully integrated with proper error handling
- [ ] Configuration system allows provider switching
- [ ] Fallback mechanism provides graceful degradation
- [ ] Response validation prevents low-quality outputs
- [ ] Integration with Telegram bot completed and tested
- [ ] All LLM operations respect ethical guidelines

---

## Next Tasks

- [ ] Task 008: Mode Analysis (depends on this LLM integration)

---

## References

- **Constitution**: `.specify/constitution.md` v0.1.1 (Ethics VIII, Resilience VII)
- **Spec**: `.specify/specs/001-core/spec.md` v2.0 (FR-008)
- **Plan**: `plan.md` Phase 3.1
- **Architecture**: `architecture-decisions.md` LLM Provider section