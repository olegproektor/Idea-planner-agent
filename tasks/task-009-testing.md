# Task 009: Testing

**Phase**: 4 - Testing & Deployment  
**Estimated Hours**: 5  
**Priority**: P1  
**Status**: Not Started

---

## Description

Implement comprehensive testing for the idea-planner-agent MVP including unit tests, integration tests, and test coverage reporting. This task ensures the application meets the >80% test coverage requirement.

---

## Acceptance Criteria

- [ ] Unit tests written for all core modules (>80% coverage) (Engineering Quality VI)
- [ ] Integration tests created for Telegram bot flow (NFR-003)
- [ ] Mock testing implemented for external APIs
- [ ] Test coverage reporting set up and working
- [ ] Test data created for all analysis modes
- [ ] All tests passing before deployment
- [ ] Test documentation completed

---

## Subtasks with Hour Estimates

| Subtask | Hours | Description |
|---------|-------|-------------|
| 9.1 Write unit tests | 2.0 | Core module unit tests (>80% coverage) |
| 9.2 Create integration tests | 1.5 | Telegram bot integration tests |
| 9.3 Implement mock testing | 0.5 | Mock external APIs for testing |
| 9.4 Set up coverage reporting | 0.5 | Test coverage measurement and reporting |
| 9.5 Create test data | 0.5 | Test data for all modes and scenarios |

---

## Dependencies

**Depends on**: 
- Task 001-008 (All implementation tasks)

**Required for**: 
- Task 010 (Deployment) - tests must pass before deployment

---

## Testing Requirements

- [ ] Verify unit tests cover >80% of codebase
- [ ] Confirm integration tests cover main user flows
- [ ] Test all 11 analysis modes
- [ ] Validate error handling in all scenarios
- [ ] Test edge cases and error conditions
- [ ] Verify test coverage reporting accuracy
- [ ] Ensure all tests pass before deployment

---

## Traceability to Constitution Principles

| Subtask | Constitution Principle | Spec Reference |
|---------|-----------------------|----------------|
| Unit tests | Engineering Quality (VI) | NFR-003 |
| Integration tests | Reality-First (III) | NFR-003 |
| Mock testing | Traceability (II) | FR-008 |
| Coverage reporting | Citations (IV) | Constitution v0.1.1 |
| Test data | Resilience (VII) | US-4 |

---

## Implementation Notes

### Testing Structure

```
tests/
â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ test_ru_search.py          # ru_search module tests
â”‚   â”œâ”€â”€ test_llm_integration.py     # LLM integration tests
â”‚   â”œâ”€â”€ test_mode_analysis.py       # Mode analysis tests
â”‚   â”œâ”€â”€ test_bot_handlers.py        # Telegram bot handler tests
â”‚   â””â”€â”€ test_database.py            # Database tests
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ test_telegram_flow.py       # Telegram bot flow tests
â”‚   â”œâ”€â”€ test_api_integration.py     # API integration tests
â”‚   â””â”€â”€ test_error_handling.py      # Error handling tests
â”œâ”€â”€ conftest.py                    # Pytest configuration
â”œâ”€â”€ test_data/                     # Test data files
â”‚   â”œâ”€â”€ sample_ideas.json           # Sample business ideas
â”‚   â”œâ”€â”€ mock_api_responses.json     # Mock API responses
â”‚   â””â”€â”€ expected_outputs/           # Expected outputs for comparison
â””â”€â”€ requirements-test.txt           # Test dependencies
```

### Unit Tests Implementation

```python
# tests/unit/test_ru_search.py
import pytest
from ru_search import search, SearchResult
from unittest.mock import patch, MagicMock

class TestRuSearch:
    """Unit tests for ru_search module"""
    
    @patch('ru_search.search._search_wildberries')
    @patch('ru_search.search._search_ozon')
    @patch('ru_search.search._search_yandex')
    def test_search_all_sources(self, mock_yandex, mock_ozon, mock_wb):
        """Test search with all sources"""
        # Mock responses
        mock_wb.return_value = {
            'source': 'wb',
            'products': [{'title': 'Product 1', 'price': '1000 â‚½'}],
            'price_range': '800-1200 â‚½',
            'citation': '[https://wb.ru, 14.12.2025 15:30, "WB test"]'
        }
        
        mock_ozon.return_value = {
            'source': 'ozon',
            'products': [{'title': 'Product 2', 'price': '1100 â‚½'}],
            'price_range': '900-1300 â‚½',
            'citation': '[https://ozon.ru, 14.12.2025 15:30, "Ozon test"]'
        }
        
        mock_yandex.return_value = {
            'source': 'yandex',
            'trends': {'monthly_searches': 1000},
            'citation': '[https://yandex.ru, 14.12.2025 15:30, "Yandex test"]'
        }
        
        result = search("test query", sources=["wb", "ozon", "yandex"])
        
        assert isinstance(result, SearchResult)
        assert result.query == "test query"
        assert len(result.sources) == 3
        assert result.sources[0].source == "wb"
        assert result.sources[1].source == "ozon"
        assert result.sources[2].source == "yandex"
    
    def test_caching(self):
        """Test caching functionality"""
        with patch('ru_search.search._search_wildberries') as mock_wb:
            mock_wb.return_value = {
                'source': 'wb',
                'products': [],
                'price_range': 'test',
                'citation': '[https://wb.ru, 14.12.2025 15:30, "test"]'
            }
            
            # First call - should not be cached
            result1 = search("test query", sources=["wb"])
            assert result1.cache_hit is False
            
            # Second call - should be cached
            result2 = search("test query", sources=["wb"])
            assert result2.cache_hit is True
            
            # Should only call mock once due to caching
            assert mock_wb.call_count == 1
```

### Integration Tests

```python
# tests/integration/test_telegram_flow.py
import pytest
from telegram import Update, Message
from telegram.ext import ContextTypes
from bot.handlers import IdeaHandler
from unittest.mock import AsyncMock, MagicMock

class TestTelegramFlow:
    """Integration tests for Telegram bot flow"""
    
    @pytest.mark.asyncio
    async def test_idea_flow(self):
        """Test complete idea analysis flow"""
        # Create mock update and context
        update = Update(1, Message(1, 1, text="ÐŸÑ€Ð¾Ð¸Ð·Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð¿Ð¾ÑÑƒÐ´Ñ‹"))
        context = MagicMock(spec=ContextTypes.DEFAULT_TYPE)
        
        # Mock handler dependencies
        handler = IdeaHandler()
        handler.report_generator.generate_report = AsyncMock(return_value=["Test report"])
        
        # Mock message methods
        mock_reply = AsyncMock()
        update.message.reply_text = mock_reply
        
        # Test flow
        await handler.handle_idea(update, context)
        
        # Verify progress message shown
        assert mock_reply.call_count >= 2
        
        # Verify report generated
        handler.report_generator.generate_report.assert_called_once()
        
        # Verify final message sent
        final_call = mock_reply.call_args_list[-1]
        assert "Test report" in final_call[0][0]
    
    @pytest.mark.asyncio
    async def test_mode_detection(self):
        """Test mode detection in messages"""
        update = Update(1, Message(1, 1, text="Ð Ð•Ð–Ð˜Ðœ: Ð‘Ð˜Ð—ÐÐ•Ð¡-ÐŸÐ›ÐÐ ÐŸÑ€Ð¾Ð¸Ð·Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð¿Ð¾ÑÑƒÐ´Ñ‹"))
        context = MagicMock(spec=ContextTypes.DEFAULT_TYPE)
        
        handler = IdeaHandler()
        handler.report_generator.generate_report = AsyncMock(return_value=["Business plan report"])
        
        mock_reply = AsyncMock()
        update.message.reply_text = mock_reply
        
        await handler.handle_idea(update, context)
        
        # Verify mode detected and passed
        call_args = handler.report_generator.generate_report.call_args
        assert call_args[0][1] == "Ð‘Ð˜Ð—ÐÐ•Ð¡-ÐŸÐ›ÐÐ"  # Mode should be passed
        
        # Verify mode mentioned in response
        final_call = mock_reply.call_args_list[-1]
        assert "Ð‘Ð˜Ð—ÐÐ•Ð¡-ÐŸÐ›ÐÐ" in final_call[0][0]
```

### Mock Testing for External APIs

```python
# tests/conftest.py
import pytest
from unittest.mock import patch, MagicMock

@pytest.fixture
def mock_wildberries_api():
    """Mock Wildberries API responses"""
    with patch('ru_search.wb.WildberriesScraper.search') as mock:
        mock.return_value = {
            'source': 'wb',
            'products': [
                {'title': 'Ð”ÐµÑ€ÐµÐ²ÑÐ½Ð½Ð°Ñ Ñ‚Ð°Ñ€ÐµÐ»ÐºÐ°', 'price': '800 â‚½', 'rating': 4.5},
                {'title': 'Ð”ÐµÑ€ÐµÐ²ÑÐ½Ð½Ð°Ñ Ð»Ð¾Ð¶ÐºÐ°', 'price': '300 â‚½', 'rating': 4.7}
            ],
            'price_range': '300-1200 â‚½',
            'citation': '[https://wb.ru/search, 14.12.2025 15:30, "WB search for Ð´ÐµÑ€ÐµÐ²ÑÐ½Ð½Ð°Ñ Ð¿Ð¾ÑÑƒÐ´Ð°"]'
        }
        yield mock

@pytest.fixture
def mock_ozon_api():
    """Mock Ozon API responses"""
    with patch('ru_search.ozon.OzonScraper.search') as mock:
        mock.return_value = {
            'source': 'ozon',
            'products': [
                {'title': 'ÐÐ°Ð±Ð¾Ñ€ Ð´ÐµÑ€ÐµÐ²ÑÐ½Ð½Ð¾Ð¹ Ð¿Ð¾ÑÑƒÐ´Ñ‹', 'price': '1500 â‚½', 'rating': 4.8},
                {'title': 'Ð”ÐµÑ€ÐµÐ²ÑÐ½Ð½Ð°Ñ Ñ‡Ð°ÑˆÐºÐ°', 'price': '500 â‚½', 'rating': 4.6}
            ],
            'price_range': '500-2000 â‚½',
            'citation': '[https://ozon.ru/search, 14.12.2025 15:30, "Ozon search for Ð´ÐµÑ€ÐµÐ²ÑÐ½Ð½Ð°Ñ Ð¿Ð¾ÑÑƒÐ´Ð°"]'
        }
        yield mock

@pytest.fixture
def mock_yandex_api():
    """Mock Yandex API responses"""
    with patch('ru_search.yandex.YandexScraper.get_trends') as mock:
        mock.return_value = {
            'source': 'yandex',
            'trends': {
                'monthly_searches': 15000,
                'trend': 'growing',
                'cpc': '45-80 â‚½',
                'competition': 'medium'
            },
            'citation': '[https://wordstat.yandex.ru, 14.12.2025 15:30, "Yandex trends for Ð´ÐµÑ€ÐµÐ²ÑÐ½Ð½Ð°Ñ Ð¿Ð¾ÑÑƒÐ´Ð°"]'
        }
        yield mock

@pytest.fixture
def mock_llm():
    """Mock LLM responses"""
    with patch('bot.llm_integration.LLMIntegration.generate_analysis') as mock:
        mock.return_value = """
        ðŸ“‹ ÐšÐÐ Ð¢ÐžÐ§ÐšÐ Ð˜Ð”Ð•Ð˜
        
        **Ð˜Ð´ÐµÑ:** ÐŸÑ€Ð¾Ð¸Ð·Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð´ÐµÑ€ÐµÐ²ÑÐ½Ð½Ð¾Ð¹ Ð¿Ð¾ÑÑƒÐ´Ñ‹
        **ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ð°:** ÐÐµÑ…Ð²Ð°Ñ‚ÐºÐ° ÑÐºÐ¾Ð»Ð¾Ð³Ð¸Ñ‡Ð½Ð¾Ð¹ Ð¿Ð¾ÑÑƒÐ´Ñ‹ Ð½Ð° Ñ€Ñ‹Ð½ÐºÐµ
        **Ð ÐµÑˆÐµÐ½Ð¸Ðµ:** ÐŸÑ€Ð¾Ð¸Ð·Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð¿Ð¾ÑÑƒÐ´Ñ‹ Ð¸Ð· Ð±ÐµÑ€ÐµÐ·Ñ‹ Ð¸ Ð´ÑƒÐ±Ð°
        **Ð¦ÐµÐ»ÐµÐ²Ð°Ñ Ð°ÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ð¸Ñ:** Ð­ÐºÐ¾-ÑÐ¾Ð·Ð½Ð°Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾Ñ‚Ñ€ÐµÐ±Ð¸Ñ‚ÐµÐ»Ð¸ 25-45 Ð»ÐµÑ‚
        **Ð Ð°Ð·Ð¼ÐµÑ€ Ñ€Ñ‹Ð½ÐºÐ°:** 10-15 Ð¼Ð»Ð½ â‚½/Ð¼ÐµÑÑÑ†
        **ÐšÐ¾Ð½ÐºÑƒÑ€ÐµÐ½Ñ‚Ñ‹:** [ÐšÐ¾Ð½ÐºÑƒÑ€ÐµÐ½Ñ‚ 1], [ÐšÐ¾Ð½ÐºÑƒÑ€ÐµÐ½Ñ‚ 2]
        **ÐœÐ¾Ð½ÐµÑ‚Ð¸Ð·Ð°Ñ†Ð¸Ñ:** ÐŸÑ€ÑÐ¼Ñ‹Ðµ Ð¿Ñ€Ð¾Ð´Ð°Ð¶Ð¸, Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¿Ð»ÐµÐ¹ÑÑ‹
        **Ð Ð¸ÑÐºÐ¸:** Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ ÐºÐ¾Ð½ÐºÑƒÑ€ÐµÐ½Ñ†Ð¸Ñ, ÑÐµÐ·Ð¾Ð½Ð½Ð¾ÑÑ‚ÑŒ
        
        ðŸ“Š ÐŸÐžÐ§Ð•ÐœÐ£ Ð¡Ð•Ð™Ð§ÐÐ¡
        
        Ð¢ÐµÐºÑƒÑ‰Ð¸Ðµ Ñ‚Ñ€ÐµÐ½Ð´Ñ‹ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ Ñ€Ð¾ÑÑ‚ ÑÐ¿Ñ€Ð¾ÑÐ° Ð½Ð° ÑÐºÐ¾-Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñ‹ Ð½Ð° 25% Ð³Ð¾Ð´ Ðº Ð³Ð¾Ð´Ñƒ.
        """
        yield mock
```

### Test Coverage Configuration

```python
# pytest.ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Coverage configuration
addopts = --cov=src --cov=bot --cov=ru_search --cov=llm --cov-report=term-missing --cov-fail-under=80

# Logging
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)s] %(message)s (%(filename)s:%(lineno)s)
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Async support
asyncio_mode = auto
```

### Test Data

```json
# tests/test_data/sample_ideas.json
{
  "ideas": [
    {
      "id": "idea_001",
      "text": "ÐŸÑ€Ð¾Ð¸Ð·Ð²Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð´ÐµÑ€ÐµÐ²ÑÐ½Ð½Ð¾Ð¹ Ð¿Ð¾ÑÑƒÐ´Ñ‹",
      "category": "Ð­ÐºÐ¾-Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñ‹",
      "expected_sections": 7,
      "test_modes": ["ÐžÐ¦Ð•ÐÐšÐ", "Ð‘Ð˜Ð—ÐÐ•Ð¡-ÐŸÐ›ÐÐ", "ÐœÐÐ ÐšÐ•Ð¢Ð˜ÐÐ“"]
    },
    {
      "id": "idea_002",
      "text": "ÐžÐ½Ð»Ð°Ð¹Ð½-ÐºÑƒÑ€ÑÑ‹ Ð¿Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÑŽ Ð´Ð»Ñ Ð´ÐµÑ‚ÐµÐ¹",
      "category": "ÐžÐ±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ",
      "expected_sections": 7,
      "test_modes": ["ÐžÐ¦Ð•ÐÐšÐ", "Ð˜Ð¡ÐŸÐžÐ›ÐÐ•ÐÐ˜Ð•", "Ð¡ÐÐ™Ð¢"]
    },
    {
      "id": "idea_003",
      "text": "Ð¡ÐµÑ€Ð²Ð¸Ñ Ð´Ð¾ÑÑ‚Ð°Ð²ÐºÐ¸ Ð·Ð´Ð¾Ñ€Ð¾Ð²Ñ‹Ñ… Ð¾Ð±ÐµÐ´Ð¾Ð²",
      "category": "FoodTech",
      "expected_sections": 7,
      "test_modes": ["ÐžÐ¦Ð•ÐÐšÐ", "ÐžÐ¢Ð§ÐÐ¢ 1", "ÐžÐ¢Ð§ÐÐ¢ 2"]
    }
  ]
}
```

### Test Execution and Reporting

```bash
# Run tests with coverage
pytest --cov=src --cov=bot --cov=ru_search --cov=llm --cov-report=html

# Generate HTML report
pytest --cov-report=html:coverage_report

# Run specific test group
pytest tests/unit/test_ru_search.py -v

# Run integration tests
pytest tests/integration/ -v
```

### Continuous Integration Setup

```yaml
# .github/workflows/test.yml
name: Run Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run unit tests
      run: pytest tests/unit/ --cov --cov-fail-under=80
    
    - name: Run integration tests
      run: pytest tests/integration/ --cov --cov-fail-under=60
    
    - name: Upload coverage report
      uses: codecov/codecov-action@v3
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        files: ./coverage.xml
```

---

## Success Criteria

- [ ] Unit tests cover >80% of core modules
- [ ] Integration tests cover main user flows
- [ ] All 11 analysis modes tested
- [ ] Error handling validated in tests
- [ ] Test coverage reporting working
- [ ] All tests passing before deployment
- [ ] Test documentation completed

---

## Next Tasks

- [ ] Task 010: Deployment (depends on successful testing)

---

## References

- **Constitution**: `.specify/constitution.md` v0.1.1 (Engineering Quality VI)
- **Spec**: `.specify/specs/001-core/spec.md` v2.0 (NFR-003)
- **Plan**: `plan.md` Phase 4.1
- **Architecture**: `architecture-decisions.md` Testing Strategy section